{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source Code Authors :**   \n",
    "> *Abigail Swamidoss (Abigail.Swamidoss@gmail.com), Samhith Kethireddy (Kethireddy.samhith@gmail.com)*  \n",
    "\n",
    "**Published in Research Article:**  \n",
    "> [Computational Analysis of Routine Biopsies Improves Diagnosis and Prediction of Cardiac Allograft Vasculopathy](https://www.ahajournals.org/doi/10.1161/CIRCULATIONAHA.121.058459)  \n",
    "> *by Eliot G. Peyster, Andrew Janowczyk, Abigail Swamidoss, Samhith Kethireddy, Michael D. Feldman and Kenneth B. Margulies*  \n",
    "*Originally published 11 Apr 2022*  \n",
    "  \n",
    "> [Supplementary Material](https://www.ahajournals.org/action/downloadSupplement?doi=10.1161%2FCIRCULATIONAHA.121.058459&file=10.1161.circulationaha.121.058459_supplemental_materials.pdf)  \n",
    "\n",
    "**Publisher**\n",
    "[Journal of American Heart Association (JAHA) - Circulation](https://www.ahajournals.org/journal/circ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Install, Setup gdrive and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import geojson\n",
    "from shapely.ops import unary_union \n",
    "from shapely import wkt\n",
    "import pandas as pd\n",
    "\n",
    "import shapely.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_CWR_SERVER = False \n",
    "slideset = 'cav'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_CWR_SERVER :\n",
    "  DATADIR = \"/somedrive/datacd31/\"\n",
    "else :\n",
    "  DATADIR = \"C:\\\\research\\\\cav\\\\datacd31\\\\\"\n",
    "\n",
    "###############################\n",
    "# RUN THIS TWICE ONCE WITH DILATE FLAG TRUE AND FALSE\n",
    "###############################\n",
    "\n",
    "# configs\n",
    "dilate = True\n",
    "#exp_nuc_4_qp = True\n",
    "save_csv = True\n",
    "HEALTHY = ['HC', 'US', 'MA']\n",
    "DESEASED = ['DC']     \n",
    "DESEASED_YR1 = ['DY'] \n",
    "COHORTS = HEALTHY + DESEASED + DESEASED_YR1\n",
    "\n",
    "#input files and folders\n",
    "JASONDIR = DATADIR + \"json\" + \"\\\\\" + slideset + \"\\\\\"\n",
    "DAB_CSV = \"21cd31_dab.csv\"\n",
    "\n",
    "#output files\n",
    "outdir = DATADIR + \"output\\\\\" + slideset + \"\\\\\"\n",
    "if dilate :\n",
    "  DAB_NUC_CSV = \"22cd31_dab_nuc_dil.csv\"\n",
    "else :\n",
    "  DAB_NUC_CSV = \"22cd31_dab_nuc_udl.csv\"\n",
    "\n",
    "#debug files\n",
    "#DAB2_CSV = \"22cd31_dab.csv\"\n",
    "#NUC2_CSV = \"22cd31_nuc.csv\"\n",
    "#NUC_CSV = \"22cd31_nuc.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtions for reading JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "#---------------------------------------------------\n",
    "def readAnnoJsonFile(ann_fname): \n",
    "  with open(ann_fname) as a:\n",
    "      annotationgeojson = geojson.load(a)\n",
    "  cd31_dab_anno_list = [obj for obj in annotationgeojson if(obj['properties']['classification']['name']== \"CD31DAB\")]\n",
    "  cd31_dab_geom_list = [shape(obj[\"geometry\"]) for obj in cd31_dab_anno_list] \n",
    "\n",
    "  cd31_hem_anno_list = [obj for obj in annotationgeojson if(obj['properties']['classification']['name']== \"CD31HEMA\")]\n",
    "  cd31_hem_geom_list = [shape(obj[\"geometry\"]) for obj in cd31_hem_anno_list] \n",
    "\n",
    "  cd31_tis_anno_list = [obj for obj in annotationgeojson if(obj['properties']['classification']['name']== \"CD31Tissue\")]\n",
    "  cd31_tis_geom_list = [shape(obj[\"geometry\"]) for obj in cd31_tis_anno_list] \n",
    "\n",
    "  return cd31_dab_geom_list, cd31_hem_geom_list, cd31_tis_geom_list\n",
    "\n",
    "#---------------------------------------------------\n",
    "def readNucJsonFile(nuc_fname): \n",
    "  with open(nuc_fname) as a:\n",
    "      nucobjects = geojson.load(a)\n",
    "\n",
    "  nucshapes=[shape(obj[\"geometry\"]) for obj in nucobjects]\n",
    "  nuccenters=[s.centroid  for s in nucshapes]\n",
    "\n",
    "  for i in range(len(nucshapes)):\n",
    "      nuccenters[i].id=i\n",
    "      nucshapes[i].id=i\n",
    "      if 'classification' in nucobjects[i].properties:\n",
    "          #label = 1 if nucobjects[i].properties[\"classification\"]['name']=='Positive' else 0\n",
    "          if nucobjects[i].properties[\"classification\"]['name']=='Positive':\n",
    "            label = 1\n",
    "          else:\n",
    "            label = 0 if nucobjects[i].properties[\"classification\"]['name']=='Negative' else -1\n",
    "      else:\n",
    "          label = -2\n",
    "      nuccenters[i].label=label\n",
    "      nucshapes[i].label=label\n",
    "      #nucshapes[i].in_dilation0 = 0\n",
    "      #nucshapes[i].in_dilation1 = 0\n",
    "      nucshapes[i].in_dab = 0\n",
    "      nucshapes[i].in_excl = 0\n",
    "      nucshapes[i].in_q1 = 0 # 25%\n",
    "      nucshapes[i].in_q2 = 0 # 50% \n",
    "      nucshapes[i].in_q3 = 0 # 75%\n",
    "      nucshapes[i].in_q4 = 0 # 100%\n",
    "      nucshapes[i].in_s1 = 0 # within +/-0.5 SD\n",
    "      nucshapes[i].in_s2 = 0 # within +/-1 SD \n",
    "      nucshapes[i].in_s3 = 0 # within +/-1.5 SD\n",
    "      nucshapes[i].in_s4 = 0 # within +/-2 SD\n",
    "      nucshapes[i].in_s5 = 0 # within +/-3 SD.\n",
    "      nucshapes[i].in_s6 = 0 # beyond +/-3 SD.\n",
    "      nucshapes[i].in_b1 = 0 # Microvessels- area between 10 µm2 and 78.4 µm2\n",
    "      nucshapes[i].in_b2 = 0 # Small pre-capillary arterioles (area 78.5–314 µm2)  \n",
    "      nucshapes[i].in_b3 = 0 # Medium veins and arterioles: area 315 um2 – 1000 um2\n",
    "      nucshapes[i].in_b4 = 0 # Medium-large: 1000-2500 um2\n",
    "      nucshapes[i].in_b5 = 0 # Large: >2500 um2\n",
    "  \n",
    "  return nucobjects, nucshapes, nuccenters\n",
    "#---------------------------------------------------\n",
    "#---------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "\n",
    "#---------------------------------------------------\n",
    "def findNucsInDAB_qrt(in_df, fname, nuc_cent_list, nuc_geom_list):\n",
    "\n",
    "  dab_q0 = in_df.loc[(in_df['fname'] == fname)]\n",
    "  dab_q1 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_q1'] > 0)]\n",
    "  dab_q2 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_q2'] > 0)]\n",
    "  dab_q3 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_q3'] > 0)]\n",
    "  dab_q4 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_q4'] > 0)]\n",
    "  \n",
    "    \n",
    "  q0_geoms = dab_q0['dab_poly'].to_numpy()\n",
    "  q1_geoms = dab_q1['dab_poly'].to_numpy()\n",
    "  q2_geoms = dab_q2['dab_poly'].to_numpy()\n",
    "  q3_geoms = dab_q3['dab_poly'].to_numpy()\n",
    "  q4_geoms = dab_q4['dab_poly'].to_numpy()\n",
    "  \n",
    "  tree = STRtree(nuc_cent_list)\n",
    "  centsinsideDAB0 = []\n",
    "  centsinsideDAB1 = []\n",
    "  centsinsideDAB2 = []\n",
    "  centsinsideDAB3 = []\n",
    "  centsinsideDAB4 = []\n",
    "\n",
    "  mpoly0 = q0_geoms # list of undilated polygons\n",
    "  if (dilate) :\n",
    "    mpoly1 = unary_union([poly.buffer(2) for poly in q1_geoms])  #dilated polygons\n",
    "    q1_area = mpoly1.area\n",
    "    mpoly2 = unary_union([poly.buffer(5) for poly in q2_geoms])  #dilated polygons\n",
    "    q2_area = mpoly2.area\n",
    "    mpoly3 = unary_union([poly.buffer(10) for poly in q3_geoms])  #dilated polygons\n",
    "    q3_area = mpoly3.area\n",
    "    mpoly4 = unary_union([poly.buffer(15) for poly in q4_geoms])  #dilated polygons\n",
    "    q4_area = mpoly4.area\n",
    "  else :\n",
    "    mpoly1 = q1_geoms\n",
    "    mpoly2 = q2_geoms\n",
    "    mpoly3 = q3_geoms\n",
    "    mpoly4 = q4_geoms\n",
    "    q1_area = sum([p.area for p in q1_geoms])\n",
    "    q2_area = sum([p.area for p in q2_geoms])\n",
    "    q3_area = sum([p.area for p in q3_geoms])\n",
    "    q4_area = sum([p.area for p in q4_geoms])\n",
    "\n",
    "  for poly_0 in mpoly0:\n",
    "    results0 = [c for c in tree.query(poly_0) if c.within(poly_0)]\n",
    "    centsinsideDAB0.extend(results0)\n",
    "  for poly_1 in mpoly1:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB1.extend(results1)\n",
    "  for poly_1 in mpoly2:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB2.extend(results1)\n",
    "  for poly_1 in mpoly3:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB3.extend(results1)\n",
    "  for poly_1 in mpoly4:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB4.extend(results1)\n",
    "    \n",
    "  for c1 in centsinsideDAB0:\n",
    "    nuc_geom_list[c1.id].in_dab = 1\n",
    "  for c1 in centsinsideDAB1:\n",
    "    nuc_geom_list[c1.id].in_q1 = 1\n",
    "  for c1 in centsinsideDAB2:\n",
    "    nuc_geom_list[c1.id].in_q2 = 1\n",
    "  for c1 in centsinsideDAB3:\n",
    "    nuc_geom_list[c1.id].in_q3 = 1\n",
    "  for c1 in centsinsideDAB4:\n",
    "    nuc_geom_list[c1.id].in_q4 = 1\n",
    "\n",
    "  \n",
    "  return nuc_geom_list, [q1_area, q2_area, q3_area, q4_area]\n",
    "#---------------------------------------------------\n",
    "def findNucsInDAB_std(in_df, fname, nuc_cent_list, nuc_geom_list):\n",
    "\n",
    "  dab_s050 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_s1'] > 0)]\n",
    "  dab_s100 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_s2'] > 0)]\n",
    "  dab_s150 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_s3'] > 0)]\n",
    "  dab_s200 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_s4'] > 0)]\n",
    "  dab_s300 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_s5'] > 0)]\n",
    "  dab_sotl = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_s6'] > 0)]\n",
    "  \n",
    "  s1_geoms = dab_s050['dab_poly'].to_numpy()\n",
    "  s2_geoms = dab_s100['dab_poly'].to_numpy()\n",
    "  s3_geoms = dab_s150['dab_poly'].to_numpy()\n",
    "  s4_geoms = dab_s200['dab_poly'].to_numpy()\n",
    "  s5_geoms = dab_s300['dab_poly'].to_numpy()\n",
    "  s6_geoms = dab_sotl['dab_poly'].to_numpy()\n",
    "\n",
    "  tree = STRtree(nuc_cent_list)\n",
    "  centsinsideDAB1 = []\n",
    "  centsinsideDAB2 = []\n",
    "  centsinsideDAB3 = []\n",
    "  centsinsideDAB4 = []\n",
    "  centsinsideDAB5 = []\n",
    "  centsinsideDAB6 = []\n",
    "\n",
    "  if (dilate) :\n",
    "    mpoly1 = unary_union([poly.buffer(2) for poly in s1_geoms]) \n",
    "    s1_area = mpoly1.area\n",
    "    if (isinstance(mpoly1, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly1 = [mpoly1]\n",
    "    mpoly2 = unary_union([poly.buffer(5) for poly in s2_geoms])  \n",
    "    s2_area = mpoly2.area\n",
    "    if (isinstance(mpoly2, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly2 = [mpoly2]\n",
    "    mpoly3 = unary_union([poly.buffer(10) for poly in s3_geoms]) \n",
    "    s3_area = mpoly3.area\n",
    "    if (isinstance(mpoly3, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly3 = [mpoly3]\n",
    "    mpoly4 = unary_union([poly.buffer(15) for poly in s4_geoms]) \n",
    "    s4_area = mpoly4.area\n",
    "    if (isinstance(mpoly4, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly4 = [mpoly4]\n",
    "    mpoly5 = unary_union([poly.buffer(20) for poly in s5_geoms]) \n",
    "    s5_area = mpoly5.area\n",
    "    if (isinstance(mpoly5, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly5 = [mpoly5]\n",
    "    mpoly6 = unary_union([poly.buffer(25) for poly in s6_geoms]) \n",
    "    s6_area = mpoly6.area\n",
    "    if (isinstance(mpoly6, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly6 = [mpoly6]\n",
    "  else :\n",
    "    mpoly1 = s1_geoms\n",
    "    mpoly2 = s2_geoms\n",
    "    mpoly3 = s3_geoms\n",
    "    mpoly4 = s4_geoms\n",
    "    mpoly5 = s5_geoms\n",
    "    mpoly6 = s6_geoms    \n",
    "    s1_area = sum([p.area for p in s1_geoms])\n",
    "    s2_area = sum([p.area for p in s2_geoms])\n",
    "    s3_area = sum([p.area for p in s3_geoms])\n",
    "    s4_area = sum([p.area for p in s4_geoms])\n",
    "    s5_area = sum([p.area for p in s5_geoms])\n",
    "    s6_area = sum([p.area for p in s6_geoms])\n",
    "\n",
    "  for poly_1 in mpoly1:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB1.extend(results1)\n",
    "  for poly_1 in mpoly2:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB2.extend(results1)\n",
    "  for poly_1 in mpoly3:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB3.extend(results1)\n",
    "  for poly_1 in mpoly4:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB4.extend(results1)\n",
    "  for poly_1 in mpoly5:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB5.extend(results1)\n",
    "  for poly_1 in mpoly6:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB6.extend(results1)\n",
    "    \n",
    "  for c1 in centsinsideDAB1:\n",
    "    nuc_geom_list[c1.id].in_s1 = 1\n",
    "  for c1 in centsinsideDAB2:\n",
    "    nuc_geom_list[c1.id].in_s2 = 1\n",
    "  for c1 in centsinsideDAB3:\n",
    "    nuc_geom_list[c1.id].in_s3 = 1\n",
    "  for c1 in centsinsideDAB4:\n",
    "    nuc_geom_list[c1.id].in_s4 = 1\n",
    "  for c1 in centsinsideDAB5:\n",
    "    nuc_geom_list[c1.id].in_s5 = 1\n",
    "  for c1 in centsinsideDAB6:\n",
    "    nuc_geom_list[c1.id].in_s6 = 1\n",
    "  \n",
    "  return nuc_geom_list, [s1_area, s2_area, s3_area, s4_area, s5_area, s6_area]\n",
    "#---------------------------------------------------\n",
    "def findNucsInDAB_excl(dab_geom_list, nuc_cent_list, nuc_geom_list):\n",
    "  tree = STRtree(nuc_cent_list)\n",
    "  centsinsideDAB0 = []\n",
    "  centsinsideDAB1 = []\n",
    "  #dilatedDABlist1 = []\n",
    "  #for dab_geom in dab_geom_list:\n",
    "  mpoly0 = dab_geom_list # list of undilated polygons\n",
    "  #mpoly1 = unary_union([poly.buffer(15) for poly in dab_geom_list])  #dilated polygons\n",
    "\n",
    "  for poly_0 in mpoly0:\n",
    "    results0 = [c for c in tree.query(poly_0) if c.within(poly_0)]\n",
    "    centsinsideDAB0.extend(results0)\n",
    "  #for poly_1 in mpoly1:\n",
    "  #  results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "  #  centsinsideDAB1.extend(results1)\n",
    "    #dilatedDABlist1.append(mpoly1)'''\n",
    "  for c0 in centsinsideDAB0:\n",
    "    nuc_geom_list[c0.id].in_excl = 1\n",
    "  #for c1 in centsinsideDAB1:\n",
    "  #  nuc_geom_list[c1.id].in_dildab_q = 1\n",
    "  \n",
    "  return nuc_geom_list\n",
    "#---------------------------------------------------\n",
    "def findNucsInDAB_bio(in_df, fname, nuc_cent_list, nuc_geom_list):\n",
    "\n",
    "  dab_b1 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_b1'] == 1)]\n",
    "  dab_b2 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_b2'] == 1)]\n",
    "  dab_b3 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_b3'] == 1)]\n",
    "  dab_b4 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_b4'] == 1)]\n",
    "  dab_b5 = in_df.loc[(in_df['fname'] == fname) & (in_df['dab_b5'] == 1)]\n",
    "    \n",
    "  b1_geoms = dab_b1['dab_poly'].to_numpy()\n",
    "  b2_geoms = dab_b2['dab_poly'].to_numpy()\n",
    "  b3_geoms = dab_b3['dab_poly'].to_numpy()\n",
    "  b4_geoms = dab_b4['dab_poly'].to_numpy()\n",
    "  b5_geoms = dab_b5['dab_poly'].to_numpy()\n",
    "  \n",
    "  tree = STRtree(nuc_cent_list)\n",
    "  centsinsideDAB1 = []\n",
    "  centsinsideDAB2 = []\n",
    "  centsinsideDAB3 = []\n",
    "  centsinsideDAB4 = []\n",
    "  centsinsideDAB5 = []\n",
    "  if (dilate) :  \n",
    "    mpoly1 = unary_union([poly.buffer(2) for poly in b1_geoms])  #dilated DAB polygons\n",
    "    b1_area = mpoly1.area\n",
    "    if (isinstance(mpoly1, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly1 = [mpoly1]\n",
    "    mpoly2 = unary_union([poly.buffer(5) for poly in b2_geoms])  #dilated DAB polygons\n",
    "    b2_area = mpoly2.area\n",
    "    if (isinstance(mpoly2, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly2 = [mpoly2]\n",
    "    mpoly3 = unary_union([poly.buffer(10) for poly in b3_geoms])  #dilated DAB polygons\n",
    "    b3_area = mpoly3.area\n",
    "    if (isinstance(mpoly3, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly3 = [mpoly3]\n",
    "    mpoly4 = unary_union([poly.buffer(15) for poly in b4_geoms])  #dilated DAB polygons\n",
    "    b4_area = mpoly4.area\n",
    "    if (isinstance(mpoly4, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly4 = [mpoly4]\n",
    "    mpoly5 = unary_union([poly.buffer(20) for poly in b5_geoms])  #dilated DAB polygons\n",
    "    b5_area = mpoly5.area\n",
    "    if (isinstance(mpoly5, shapely.geometry.polygon.Polygon)):\n",
    "        mpoly5 = [mpoly5]\n",
    "  else :\n",
    "    mpoly1 = b1_geoms\n",
    "    mpoly2 = b2_geoms\n",
    "    mpoly3 = b3_geoms\n",
    "    mpoly4 = b4_geoms\n",
    "    mpoly5 = b5_geoms  \n",
    "    b1_area = sum([p.area for p in b1_geoms])\n",
    "    b2_area = sum([p.area for p in b2_geoms])\n",
    "    b3_area = sum([p.area for p in b3_geoms])\n",
    "    b4_area = sum([p.area for p in b4_geoms])\n",
    "    b5_area = sum([p.area for p in b5_geoms])\n",
    "\n",
    "  for poly_1 in mpoly1:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB1.extend(results1)\n",
    "  for poly_1 in mpoly2:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB2.extend(results1)\n",
    "  for poly_1 in mpoly3:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB3.extend(results1)\n",
    "  for poly_1 in mpoly4:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB4.extend(results1)\n",
    "  for poly_1 in mpoly5:\n",
    "    results1 = [c for c in tree.query(poly_1) if c.within(poly_1)]\n",
    "    centsinsideDAB5.extend(results1)\n",
    "    \n",
    "  for c1 in centsinsideDAB1:\n",
    "    nuc_geom_list[c1.id].in_b1 = 1\n",
    "  for c1 in centsinsideDAB2:\n",
    "    nuc_geom_list[c1.id].in_b2 = 1\n",
    "  for c1 in centsinsideDAB3:\n",
    "    nuc_geom_list[c1.id].in_b3 = 1\n",
    "  for c1 in centsinsideDAB4:\n",
    "    nuc_geom_list[c1.id].in_b4 = 1\n",
    "  for c1 in centsinsideDAB5:\n",
    "    nuc_geom_list[c1.id].in_b5 = 1\n",
    "  \n",
    "  return nuc_geom_list, [b1_area, b2_area, b3_area, b4_area, b5_area]\n",
    "#---------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DAB CSV & Determine DAB BIN boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{outdir}{DAB_CSV}.zip\"):\n",
    "      print(f\"reading file: {outdir}{DAB_CSV}.zip\")\n",
    "      raw_dab_df=pd.read_csv(f\"{outdir}{DAB_CSV}.zip\")\n",
    "\n",
    "# eliminate errors from QuPath\n",
    "error_margin_percentile = raw_dab_df.dab_area.quantile(0.99) \n",
    "dab_df = pd.DataFrame(raw_dab_df[raw_dab_df.dab_area < error_margin_percentile])\n",
    "\n",
    "# boundaries for Quartiles\n",
    "dstats = dab_df.describe()\n",
    "q25 = dstats.loc['25%'].values[0]\n",
    "q50 = dstats.loc['50%'].values[0]\n",
    "q75 = dstats.loc['75%'].values[0]\n",
    "#boundaries_m1 = [0, q25, q50, q75]\n",
    "#print(f'quartiles 25% = {q25}, 50% = {q50}, 75% = {q75}')\n",
    "\n",
    "# calculate zscore\n",
    "mean = dab_df['dab_area'].mean()\n",
    "std = dab_df['dab_area'].std()\n",
    "dab_df['dab_zscore'] = (dab_df['dab_area']-mean)/std\n",
    "\n",
    "# mark dab area above 0.05% as outliers\n",
    "otl_percentile = dab_df.dab_area.quantile(.995)\n",
    "dab_df[\"dab_ex\"] = np.where(dab_df[\"dab_area\"] < otl_percentile, 0, 1)\n",
    "\n",
    "# quartile based bins\n",
    "dab_df[\"dab_q1\"] = np.where(dab_df[\"dab_area\"] < q25 ,1,0)\n",
    "dab_df[\"dab_q2\"] = np.where((dab_df[\"dab_area\"] >= q25) & (dab_df[\"dab_area\"] < q50), 1, 0)\n",
    "dab_df[\"dab_q3\"] = np.where((dab_df[\"dab_area\"] >= q50) & (dab_df[\"dab_area\"] < q75), 1, 0)\n",
    "dab_df[\"dab_q4\"] = np.where(dab_df[\"dab_area\"] >= q75, 1, 0)\n",
    "\n",
    "# sd based bins\n",
    "P050 = 0.50000000000000001\n",
    "P100 = 1.00000000000000001\n",
    "P150 = 1.50000000000000001\n",
    "P200 = 2.00000000000000001\n",
    "P300 = 3.00000000000000001\n",
    "dab_df[\"dab_s1\"] = np.where( dab_df['dab_zscore'].between(-P050, 0.5), 1,0)\n",
    "dab_df[\"dab_s2\"] = np.where( dab_df['dab_zscore'].between(-P100, -0.5) | dab_df['dab_zscore'].between(P050, 1.0) ,1,0)\n",
    "dab_df[\"dab_s3\"] = np.where( dab_df['dab_zscore'].between(-P150, -1.0) | dab_df['dab_zscore'].between(P100, 1.5) ,1,0)\n",
    "dab_df[\"dab_s4\"] = np.where( dab_df['dab_zscore'].between(-P200, -1.5) | dab_df['dab_zscore'].between(P150, 2.0) ,1,0)\n",
    "dab_df[\"dab_s5\"] = np.where( dab_df['dab_zscore'].between(-P300, -2.0) | dab_df['dab_zscore'].between(P200, 3.0) ,1,0)\n",
    "dab_df[\"dab_s6\"] = np.where( (dab_df['dab_zscore'] < -3.0) | (dab_df['dab_zscore'] > 3), 1,0)\n",
    "\n",
    "# biological  bins\n",
    "B = [158, 1239, 4964, 15809, 39524] # m2 [10, 78.5, 315, 1000, 2500] \n",
    "dab_df[\"dab_b1\"] = np.where(dab_df[\"dab_area\"] < B[1], 1, 0)\n",
    "dab_df[\"dab_b2\"] = np.where((dab_df[\"dab_area\"] >= B[1]) & (dab_df[\"dab_area\"] < B[2]), 1, 0)\n",
    "dab_df[\"dab_b3\"] = np.where((dab_df[\"dab_area\"] >= B[2]) & (dab_df[\"dab_area\"] < B[3]), 1, 0)\n",
    "dab_df[\"dab_b4\"] = np.where((dab_df[\"dab_area\"] >= B[3]) & (dab_df[\"dab_area\"] < B[4]), 1, 0)\n",
    "dab_df[\"dab_b5\"] = np.where(dab_df[\"dab_area\"] >= B[4], 1, 0)\n",
    "\n",
    "\n",
    "#save_df = dab_df.drop(columns=['dab_poly'])\n",
    "#print(f\"saving: {outdir}{DAB2_CSV}.zip\")\n",
    "#save_df.to_csv(f\"{outdir}{DAB2_CSV}.zip\", index=False, compression=dict(method='zip', archive_name=f'{DAB2_CSV}'))\n",
    "\n",
    "#save_df.to_csv(DAB2_CSV, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop: It loops through DABs grouped by fname, identifies and marks nucs by various DAB bins (qrt, std, bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_df = dab_df\n",
    "\n",
    "# for testing start #########\n",
    "#testfiles = ['CAV-DC-1', 'CAV-DC-2', 'CAV-DC-11', 'CAV-DC-12', 'CAV-DC-19','CAV-DY1-10', 'CAV-HC-18', 'CAV-DY1-2', 'CAV-MATCH-5', 'CAV-MATCH-5 ']\n",
    "#testfiles = ['CAV2-DC-10_HS-17-0034099'] #'CAV2-DC-12_HS-18-0041740', 'CAV2-DC-29_HS-17-0019977', 'CAV2-HC-9_HS-15-0031776', 'CAV2-DY1-11_HS-13-0022491', 'CAV2-DY1-6_HS-16-0010050']\n",
    "#d_df = dab_df.loc[dab_df['fname'].isin(testfiles)]\n",
    "# for testing end #########\n",
    "\n",
    "d_df['dab_poly'] = d_df['dab_poly'].apply(wkt.loads)\n",
    "\n",
    "nuc_df=pd.DataFrame()\n",
    "dil_df=pd.DataFrame()\n",
    "grouped = d_df.groupby(['cohort','fname'])\n",
    "# iterate over each group\n",
    "for name, group in grouped:\n",
    "    cohort = name[0]\n",
    "    fname = name[1]\n",
    "    #dab_geom_list = group.dab_poly.to_numpy()\n",
    "    ex_df = group.loc[(group['dab_ex'] == 1)] # excluded dabs >0.05 %\n",
    "    in_df = group.loc[(group['dab_ex'] == 0)] # selected dabs < 0.995 pcile\n",
    "    dab_geom_list = in_df.dab_poly.to_numpy()\n",
    "    ex_dab_geom_list = ex_df.dab_poly.to_numpy()\n",
    "\n",
    "\n",
    "    nnn, nuc_geom_list, nuc_cent_list = readNucJsonFile(JASONDIR + fname +'_nuc.json') \n",
    "    #print(f\"{fname}: dab.size={len(dab_geom_list)}, nuc.size={len(nuc_geom_list)}\")\n",
    "\n",
    "    #print(len(nuc_geom_list))\n",
    "    ################# Quartile bins\n",
    "    nuc_geom_list, dil_d_q = findNucsInDAB_qrt(in_df, fname, nuc_cent_list, nuc_geom_list)\n",
    "\n",
    "    ################# SD bins\n",
    "    nuc_geom_list, dil_d_s = findNucsInDAB_std(in_df, fname, nuc_cent_list, nuc_geom_list)\n",
    "\n",
    "    ############## outliers\n",
    "    nuc_geom_list = findNucsInDAB_excl(ex_dab_geom_list, nuc_cent_list, nuc_geom_list)\n",
    "\n",
    "    ############## Bilogical \n",
    "    nuc_geom_list, dil_d_b = findNucsInDAB_bio(in_df, fname, nuc_cent_list, nuc_geom_list)\n",
    "\n",
    "    ############## \n",
    "    temp_n_df = pd.DataFrame({'cohort':cohort,'fname':fname,  \n",
    "                            'in_dab': [nuc.in_dab  for nuc in nuc_geom_list], \n",
    "                            'in_q1': [nuc.in_q1  for nuc in nuc_geom_list], \n",
    "                            'in_q2': [nuc.in_q2  for nuc in nuc_geom_list], \n",
    "                            'in_q3': [nuc.in_q3  for nuc in nuc_geom_list], \n",
    "                            'in_q4': [nuc.in_q4  for nuc in nuc_geom_list], \n",
    "                            'in_s1': [nuc.in_s1  for nuc in nuc_geom_list], \n",
    "                            'in_s2': [nuc.in_s2  for nuc in nuc_geom_list], \n",
    "                            'in_s3': [nuc.in_s3  for nuc in nuc_geom_list], \n",
    "                            'in_s4': [nuc.in_s4  for nuc in nuc_geom_list], \n",
    "                            'in_s5': [nuc.in_s5  for nuc in nuc_geom_list], \n",
    "                            'in_s6': [nuc.in_s6  for nuc in nuc_geom_list],\n",
    "                            'in_b1': [nuc.in_b1  for nuc in nuc_geom_list], \n",
    "                            'in_b2': [nuc.in_b2  for nuc in nuc_geom_list], \n",
    "                            'in_b3': [nuc.in_b3  for nuc in nuc_geom_list], \n",
    "                            'in_b4': [nuc.in_b4  for nuc in nuc_geom_list], \n",
    "                            'in_b5': [nuc.in_b5  for nuc in nuc_geom_list], \n",
    "                            'in_excl': [nuc.in_excl for nuc in nuc_geom_list],\n",
    "                            'nuc_area' : [nuc.area  for nuc in nuc_geom_list]\n",
    "                            })\n",
    "    nuc_df = nuc_df.append(temp_n_df)\n",
    "\n",
    "    temp_dil_df = pd.DataFrame({'cohort':cohort,'fname':fname,  \n",
    "                            'dil_d_q1_area':[dil_d_q[0]],\n",
    "                            'dil_d_q2_area':[dil_d_q[1]],\n",
    "                            'dil_d_q3_area':[dil_d_q[2]],\n",
    "                            'dil_d_q4_area':[dil_d_q[3]],\n",
    "                            'dil_d_s1_area':[dil_d_s[0]],\n",
    "                            'dil_d_s2_area':[dil_d_s[1]],\n",
    "                            'dil_d_s3_area':[dil_d_s[2]],\n",
    "                            'dil_d_s4_area':[dil_d_s[3]],\n",
    "                            'dil_d_s5_area':[dil_d_s[4]],\n",
    "                            'dil_d_s6_area':[dil_d_s[5]],\n",
    "                            'dil_d_b1_area':[dil_d_b[0]],\n",
    "                            'dil_d_b2_area':[dil_d_b[1]],\n",
    "                            'dil_d_b3_area':[dil_d_b[2]],\n",
    "                            'dil_d_b4_area':[dil_d_b[3]],\n",
    "                            'dil_d_b5_area':[dil_d_b[4]]\n",
    "                            })\n",
    "    dil_df = dil_df.append(temp_dil_df)\n",
    "\n",
    "    #break\n",
    "    #end loop\n",
    "\n",
    "#print(f\"saving {outdir}{NUC2_CSV}.zip\")\n",
    "##nuc_df.to_csv(NUC2_CSV, index=False)\n",
    "#nuc_df.to_csv(f\"{outdir}{NUC2_CSV}.zip\", index=False, compression=dict(method='zip', archive_name=f'{NUC2_CSV}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(f\"saving {outdir}{NUC2_CSV}.zip\")\n",
    "##nuc_df.to_csv(NUC2_CSV, index=False)\n",
    "#nuc_df.to_csv(f\"{outdir}{NUC2_CSV}.zip\", index=False, compression=dict(method='zip', archive_name=f'{NUC2_CSV}'))\n",
    "\n",
    "########### DAB\n",
    "df11 = d_df.groupby(['cohort','fname']).size().reset_index(name=\"dab_count\")\n",
    "df12 = d_df.loc[(d_df['dab_ex'] == 1)]\n",
    "df12 = df12.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_ex_area\")\n",
    "df13 = d_df.groupby(['cohort','fname'])[['dab_area', 'dab_ex', 'dab_q1',  'dab_q2',  'dab_q3',  'dab_q4',  'dab_s1',   'dab_s2',   'dab_s3',   'dab_s4', 'dab_s5', 'dab_s6', 'dab_b1',  'dab_b2',  'dab_b3',  'dab_b4', 'dab_b5']].sum()\n",
    "\n",
    "df14 = d_df.loc[(d_df['dab_q1'] == 1)]\n",
    "df14 = df14.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_q1_area\")\n",
    "df15 = d_df.loc[(d_df['dab_q2'] == 1)]\n",
    "df15 = df15.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_q2_area\")\n",
    "df16 = d_df.loc[(d_df['dab_q3'] == 1)]\n",
    "df16 = df16.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_q3_area\")\n",
    "df17 = d_df.loc[(d_df['dab_q4'] == 1)]\n",
    "df17 = df17.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_q4_area\")\n",
    "\n",
    "df18 = d_df.loc[(d_df['dab_s1'] == 1)]\n",
    "df18 = df18.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_s1_area\")\n",
    "df19 = d_df.loc[(d_df['dab_s2'] == 1)]\n",
    "df19 = df19.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_s2_area\")\n",
    "df20 = d_df.loc[(d_df['dab_s3'] == 1)]\n",
    "df20 = df20.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_s3_area\")\n",
    "df21 = d_df.loc[(d_df['dab_s4'] == 1)]\n",
    "df21 = df21.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_s4_area\")\n",
    "df22 = d_df.loc[(d_df['dab_s5'] == 1)]\n",
    "df22 = df22.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_s5_area\")\n",
    "df23 = d_df.loc[(d_df['dab_s6'] == 1)]\n",
    "df23 = df23.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_s6_area\")\n",
    "\n",
    "df24 = d_df.loc[(d_df['dab_b1'] == 1)]\n",
    "df24 = df24.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_b1_area\")\n",
    "df25 = d_df.loc[(d_df['dab_b2'] == 1)]\n",
    "df25 = df25.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_b2_area\")\n",
    "df26 = d_df.loc[(d_df['dab_b3'] == 1)]\n",
    "df26 = df26.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_b3_area\")\n",
    "df27 = d_df.loc[(d_df['dab_b4'] == 1)]\n",
    "df27 = df27.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_b4_area\")\n",
    "df28 = d_df.loc[(d_df['dab_b5'] == 1)]\n",
    "df28 = df28.groupby(['cohort','fname'])['dab_area'].sum().reset_index(name=\"dab_b5_area\")\n",
    "\n",
    "############  DILATED DAB\n",
    "df40 = dil_df.groupby(['cohort','fname'])[['dil_d_q1_area','dil_d_q2_area','dil_d_q3_area','dil_d_q4_area','dil_d_s1_area','dil_d_s2_area','dil_d_s3_area','dil_d_s4_area','dil_d_s5_area','dil_d_s6_area', 'dil_d_b1_area','dil_d_b2_area','dil_d_b3_area','dil_d_b4_area','dil_d_b5_area']].sum()\n",
    "\n",
    "############  NUC\n",
    "df54 = nuc_df.groupby(['cohort','fname']).size().reset_index(name=\"nuc_count\")\n",
    "df55 = nuc_df.groupby(['cohort','fname'])[['in_dab', 'in_q1', 'in_q2', 'in_q3', 'in_q4', 'in_s1', 'in_s2', 'in_s3', 'in_s4', 'in_s5', 'in_s6', 'in_b1', 'in_b2', 'in_b3', 'in_b4', 'in_b5', 'in_excl', 'nuc_area']].sum()\n",
    "\n",
    "df56 = nuc_df.loc[(nuc_df['in_s1'] == 1)]\n",
    "df56 = df56.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_s1_n_area\")\n",
    "df57 = nuc_df.loc[(nuc_df['in_s2'] == 1)]\n",
    "df57 = df57.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_s2_n_area\")\n",
    "df58 = nuc_df.loc[(nuc_df['in_s3'] == 1)]\n",
    "df58 = df58.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_s3_n_area\")\n",
    "df59 = nuc_df.loc[(nuc_df['in_s4'] == 1)]\n",
    "df59 = df59.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_s4_n_area\")\n",
    "df60 = nuc_df.loc[(nuc_df['in_s5'] == 1)]\n",
    "df60 = df60.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_s5_n_area\")\n",
    "df61 = nuc_df.loc[(nuc_df['in_s6'] == 1)]\n",
    "df61 = df61.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_s6_n_area\")\n",
    "df62 = nuc_df.loc[(nuc_df['in_excl'] == 1)]\n",
    "df62 = df62.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_excl_n_area\")\n",
    "\n",
    "df63 = nuc_df.loc[(nuc_df['in_b1'] == 1)]\n",
    "df63 = df63.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_b1_n_area\")\n",
    "df64 = nuc_df.loc[(nuc_df['in_b2'] == 1)]\n",
    "df64 = df64.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_b2_n_area\")\n",
    "df65 = nuc_df.loc[(nuc_df['in_b3'] == 1)]\n",
    "df65 = df65.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_b3_n_area\")\n",
    "df66 = nuc_df.loc[(nuc_df['in_b4'] == 1)]\n",
    "df66 = df66.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_b4_n_area\")\n",
    "df67 = nuc_df.loc[(nuc_df['in_b5'] == 1)]\n",
    "df67 = df67.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_b5_n_area\")\n",
    "\n",
    "df68 = nuc_df.loc[(nuc_df['in_q1'] == 1)]\n",
    "df68 = df68.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_q1_n_area\")\n",
    "df69 = nuc_df.loc[(nuc_df['in_q2'] == 1)]\n",
    "df69 = df69.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_q2_n_area\")\n",
    "df70 = nuc_df.loc[(nuc_df['in_q3'] == 1)]\n",
    "df70 = df70.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_q3_n_area\")\n",
    "df71 = nuc_df.loc[(nuc_df['in_q4'] == 1)]\n",
    "df71 = df71.groupby(['cohort','fname'])['nuc_area'].sum().reset_index(name=\"in_q4_n_area\")\n",
    "\n",
    "\n",
    "df = pd.merge(left=df11, right=df12, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df13, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df14, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df15, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df16, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df17, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df18, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df19, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df20, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df21, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df22, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df23, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df24, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df25, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df26, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df27, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df28, on=['cohort', 'fname'], how=\"outer\")\n",
    "\n",
    "df = pd.merge(left=df, right=df40, on=['cohort', 'fname'], how=\"outer\")\n",
    "\n",
    "df = pd.merge(left=df, right=df54, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df55, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df56, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df57, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df58, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df59, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df60, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df61, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df62, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df63, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df64, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df65, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df66, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df67, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df68, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df69, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df70, on=['cohort', 'fname'], how=\"outer\")\n",
    "df = pd.merge(left=df, right=df71, on=['cohort', 'fname'], how=\"outer\")\n",
    "\n",
    "print(f'saving {outdir}{DAB_NUC_CSV}.zip')\n",
    "#df.to_csv(DAB_NUC_CSV, index=False)\n",
    "df.to_csv(f\"{outdir}{DAB_NUC_CSV}.zip\", index=False, compression=dict(method='zip', archive_name=f'{DAB_NUC_CSV}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eef35e2da80e08d457f207eb475c9c1202eab50e3e1bde5e2984b2d7f2af5721"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "eef35e2da80e08d457f207eb475c9c1202eab50e3e1bde5e2984b2d7f2af5721"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
